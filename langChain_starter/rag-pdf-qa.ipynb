{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e695dc29bb9a3a",
   "metadata": {},
   "source": [
    "# Simple RAG pipeline allowing you to \"talk\" to your documentation\n",
    "\n",
    "This notebook contains a simple application for using retrieval augmented generation (RAG) to \"ask questions\" from a PDF, using a powerful package called `langchain`. In this case, we're going to use a PDF of the PyCharm documentation, but `langchain` allows you to use a [wide variety of input formats](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/), giving you significant flexibility over your input data source.\n",
    "\n",
    "In this pipeline, we'll need to do the following:\n",
    "* Load in (for local models) or connect to the API of (for remote models) our LLM;\n",
    "* Load in our PDF that we want to \"chat\" to;\n",
    "* We can't pass the whole PDF into a model at the same time (it's almost 2000 pages!). As such, we need to split it into chunks;\n",
    "* Rather than needing to pass every individual chunk through the LLM to find the information in the document relevant to a question, we can convert these chunks into document embeddings, which we then store in a vector database. At query time, the question is also converted into a document embedding, and the most similar document chunks to the question are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a98011df002e3ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T10:57:27.911921Z",
     "start_time": "2024-06-05T10:57:27.746506Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain import chains, document_loaders, vectorstores\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "import re\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5853364f53f165",
   "metadata": {},
   "source": [
    "## Count the number of pages in the PDF\n",
    "\n",
    "As you can see, we have a lot of documentation to sort through here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad45b37d3f23ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T10:57:33.371323Z",
     "start_time": "2024-06-05T10:57:31.690904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = PyPDF2.PdfReader(open(\"./materials/test_paper.pdf\", \"rb\"))\n",
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c1a3bcaaee5c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:47:12.778389Z",
     "start_time": "2024-05-31T15:47:12.770226Z"
    }
   },
   "outputs": [],
   "source": [
    "class PdfQA:\n",
    "    \"\"\"\n",
    "    Initializes the PdfQA class with the specified parameters.\n",
    "\n",
    "    :param model: The name or path of the model to be loaded.\n",
    "    :param pdf_document: The path to the PDF document to be loaded.\n",
    "    :param chunk_size: The desired size of each chunk.\n",
    "    :param chunk_overlap: The specified overlap between chunks.\n",
    "    :param search_type: The type of search to be performed.\n",
    "    :param n_documents: The number of documents to be retrieved.\n",
    "    :param chain_type: The type of chain to create.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, pdf_document, chunk_size, chunk_overlap,\n",
    "                 search_type, n_documents, chain_type):\n",
    "        #load_dotenv()\n",
    "        self.init_chat_model(model)\n",
    "        self.load_documents(pdf_document)\n",
    "        self.split_documents(chunk_size, chunk_overlap)\n",
    "        self.select_embedding = OpenAIEmbeddings()\n",
    "        self.create_vectorstore()\n",
    "        self.create_retriever(search_type, n_documents)\n",
    "        self.chain = self.create_chain(chain_type)\n",
    "\n",
    "    def init_chat_model(self, model):\n",
    "        \"\"\"\n",
    "        Initialize the chat model.\n",
    "\n",
    "        :param model: The name or path of the model to be loaded.\n",
    "        :return: None\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Loading model\")\n",
    "        self.llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "\n",
    "    def load_documents(self, pdf_document):\n",
    "        \"\"\"\n",
    "        Load documents from a PDF file and convert to a format that can be ingested by the langchain\n",
    "        document splitter.\n",
    "\n",
    "        :param pdf_document: The path to the PDF document to be loaded.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Loading PDFs\")\n",
    "        pdf_loader = document_loaders.PyPDFLoader(pdf_document)\n",
    "        self.documents = pdf_loader.load()\n",
    "\n",
    "    def split_documents(self, chunk_size, chunk_overlap):\n",
    "        \"\"\"\n",
    "        Split the documents into chunks of a given size with a specified overlap.\n",
    "\n",
    "        :param chunk_size: The desired size of each chunk.\n",
    "        :type chunk_size: int\n",
    "        :param chunk_overlap: The specified overlap between chunks.\n",
    "        :type chunk_overlap: int\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Splitting documents\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        self.texts = text_splitter.split_documents(self.documents)\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        \"\"\"\n",
    "        Create Vector Store.\n",
    "\n",
    "        This method creates document embeddings using the Chroma algorithm from the given texts and selected embedding.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Creating document embeddings\")\n",
    "        self.db = vectorstores.Chroma.from_documents(self.texts, self.select_embedding)\n",
    "\n",
    "    def create_retriever(self, search_type, n_documents):\n",
    "        \"\"\"\n",
    "        Generate a chunk retriever for the given search type and number of documents.\n",
    "\n",
    "        :param search_type: The type of search to be performed.\n",
    "        :param n_documents: The number of documents to be retrieved.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Generating chunk retriever\")\n",
    "        self.retriever = self.db.as_retriever(search_type=search_type, search_kwargs={\"k\": n_documents})\n",
    "\n",
    "    def create_chain(self, chain_type):\n",
    "        \"\"\"\n",
    "        :param chain_type: The type of chain to create.\n",
    "        :return: The created chain.\n",
    "        \"\"\"\n",
    "        qa = chains.RetrievalQA.from_chain_type(llm=self.llm,\n",
    "                                                chain_type=chain_type,\n",
    "                                                retriever=self.retriever,\n",
    "                                                return_source_documents=True)\n",
    "        return qa\n",
    "\n",
    "    def query_chain(self):\n",
    "        \"\"\"\n",
    "        Returns the chain of the object.\n",
    "\n",
    "        :return: The chain of the object.\n",
    "        \"\"\"\n",
    "        return self.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a752314ad724",
   "metadata": {},
   "source": [
    "## Levers in the RAG pipeline\n",
    "RAG is quite tricky to get right, especially if you need it to be efficient. There are many levers we can pull in our pipeline, which influence the following things:\n",
    "* How fast we can get our answers;\n",
    "* How relevant our answers are (and related, how likely we are to get a hallucination);\n",
    "* How complete our answers are.\n",
    "\n",
    "Let's instantiate our PDF questioner with the following values:\n",
    "* `model`: the LLM used to generate answers using information from the document. In this case, `gpt-3.5-turbo`.\n",
    "* `pdf_document`: the PDF we want to \"chat with\". In our case, we've selected our PDF containing almost all of the PyCharm documentation.\n",
    "* `chunk_size`: the maximum number of tokens to include in each chunk. We've selected 1000.\n",
    "* `chunk_overlap`: the number of tokens that should overlap between adjacent chunks. We've selected 0, so no overlapping tokens.\n",
    "* `search_type`: the metric by which chunks are selected. In this case, we've selected \"similarity\", so those chunks with the highest (cosine) similarity to the content of the question we're asking. However, you can also use \"mmr\" (if supported by your document store) which tries to maximise for relevancy and diversity of results.\n",
    "* `n_documents`: the maximum number of chunks to use to generate the answer. In this case, we've used 5.\n",
    "* `chain_type`: this controls how the content is passed into the LLM. In the case of \"stuff\" it passes all gathered context chunks into the context window at once. Other options are \"refine\", which feeds in the chunks in batches, plus the answer generated so far, and \"map-rerank\", which feeds in each chunk and assigns a score based on how well it answered the question.\n",
    "\n",
    "Other levers I've chosen not to make arguments in this class are the model used for embeddings (the `OpenAIEmbeddings` were used) and which vector database we use to store the document embeddings (in this case, the `Chroma` vector store was used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "696e06baa6306eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:48:30.658710Z",
     "start_time": "2024-05-31T15:47:15.228269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading PDFs\n",
      "Splitting documents\n",
      "Creating document embeddings\n",
      "Generating chunk retriever\n"
     ]
    }
   ],
   "source": [
    "pdf_qa = PdfQA(\"gpt-3.5-turbo\", \"./materials/test_paper.pdf\", 1000, 0, \"similarity\", \n",
    "               5, \"stuff\")\n",
    "pdf_qa_chain = pdf_qa.query_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1618a7539d451a0",
   "metadata": {},
   "source": [
    "Let's try it out by asking how we can debug in PyCharm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec7e8f62e1ff0803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:48:57.589492Z",
     "start_time": "2024-05-31T15:48:54.552619Z"
    }
   },
   "outputs": [],
   "source": [
    "answer1 = pdf_qa_chain.invoke({\"query\": \"what is an RNN\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1fbc81d231df98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:00.235137Z",
     "start_time": "2024-05-31T15:49:00.229727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Recurrent Neural Network (RNN) is a type of neural network that is designed to handle sequential data by maintaining a memory of previous inputs. It has connections that form a directed cycle, allowing information to persist. RNNs are commonly used in tasks like language modeling, speech recognition, and time series prediction.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f4f9b2ba9b26",
   "metadata": {},
   "source": [
    "We can see the answer is very comprehensive. Let's have a look at the information it was based on from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1453f0dff1fe05d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:04.163413Z",
     "start_time": "2024-05-31T15:49:04.159068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOCUMENT 1\n",
      "Under review as a conference paper at ICLR 2016 2 R ELATED WORK Recurrent Networks . Recurrent Neural Networks (RNNs) have a long history of applications in various sequence learning tasks Werbos (1988); Schmidhuber (2015); Rumelhart et al. (1985). De- spite their early successes, the difﬁculty of training simple recurrent networks Bengio et al. (1994); Pascanu et al. (2012) has encouraged various proposals for improvements to their basic architec- ture. Among the most successful variants are the Long Short Term Memory networks Hochreiter & Schmidhuber (1997), which can in principle store and retrieve information over long time periods with explicit gating mechanisms and a built-in constant error carousel. In the recent years there has been a renewed interest in further improving on the basic architecture by modifying the functional form as seen with Gated Recurrent Units Cho et al. (2014), incorporating content-based soft atten- tion mechanisms Bahdanau et al. (2014); Weston et al. (2014), push-pop stacks Joulin & Mikolov (2015), or more generally external memory arrays with both content-based and relative addressing mechanisms Graves et al. (2014). In this work we focus the majority of our analysis on the LSTM due to its widespread popularity and a proven track record. Understanding Recurrent Networks . While there is an abundance of work that modiﬁes or extends the basic LSTM architecture, relatively little attention has been paid to understanding the properties of its representations and predictions. Greff et al. (2015) recently conducted a comprehensive study of LSTM components. Chung et al. evaluated GRU compared to LSTMs Chung et al. (2014). Joze- fowicz et al. (2015) conduct an automated architecture search of thousands of RNN architectures. Pascanu et al. (2013) examined the effects of depth . These approaches study recurrent network based only on the variations in the ﬁnal test set cross entropy, while we break down the performance into interpretable categories and study individual error types. Most related to our work is Hermans & Schrauwen (2013), who also study the long-term interactions learned by recurrent networks in the context of character-level language models, speciﬁcally in the context of parenthesis closing and time-scales analysis. Our work complements their results and provides additional types of analysis. Lastly, we are heavily inﬂuenced by work on in-depth analysis of errors in object detection Hoiem et al. (2012), where the ﬁnal mean average precision is similarly broken down and studied in detail. 3 E XPERIMENTAL SETUP We ﬁrst describe three commonly used recurrent network architectures (RNN, LSTM and the GRU), then describe their used in sequence learning and ﬁnally discuss the optimization. 3.1 R ECURRENT NEURAL NETWORK MODELS The simplest instantiation of a deep recurrent network arranges hidden state vectors hl tin a two- dimensional grid, where t= 1. . . T is thought of as time and l= 1. . . L is the depth. The bottom row of vectors h0 t=xtat depth zero holds the input vectors xtand each vector in the top row {hL t} is used to predict an output vector yt. All intermediate vectors hl tare computed with a recurrence formula based on hl t−1andhl−1 t. Through these hidden vectors, each output ytat time step t becomes a function of all input vectors up to t,{x1, . . . , x t}. The precise mathematical form of the recurrence (hl t−1,hl−1 t)→hl tvaries from model to model and we describe these details next. Vanilla Recurrent Neural Network (RNN) has a recurrence of the form hl t= tanh Wl( hl−1 t hl t−1) where we assume that all h∈Rn. The parameter matrix Wlon each layer has dimensions [ n×2n] andtanh is applied elementwise. Note that Wlvaries between layers but is shared through time. We omit the bias vectors for brevity. Interpreting the equation above, the inputs from the layer below in depth ( hl−1 t) and before in time ( hl t−1) are transformed and interact through additive interaction before being squashed by tanh . This is known to be a weak form of coupling Sutskever et al. (2011). Both the LSTM and the GRU (discussed next) include more powerful multiplicative interactions. Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997) was designed to address the difﬁculties of training RNNs Bengio et al. (1994). In particular, it was observed that the back- propagation dynamics caused the gradients in an RNN to either vanish or explode. It was later found that the exploding gradient concern can be alleviated with a heuristic of clipping the gradients at some maximum value Pascanu et al. (2012). On the other hand, LSTMs were designed to miti- gate the vanishing gradient problem. In addition to a hidden state vector hl t, LSTMs also maintain a 2\n",
      "\n",
      "DOCUMENT 1\n",
      "Under review as a conference paper at ICLR 2016 2 R ELATED WORK Recurrent Networks . Recurrent Neural Networks (RNNs) have a long history of applications in various sequence learning tasks Werbos (1988); Schmidhuber (2015); Rumelhart et al. (1985). De- spite their early successes, the difﬁculty of training simple recurrent networks Bengio et al. (1994); Pascanu et al. (2012) has encouraged various proposals for improvements to their basic architec- ture. Among the most successful variants are the Long Short Term Memory networks Hochreiter & Schmidhuber (1997), which can in principle store and retrieve information over long time periods with explicit gating mechanisms and a built-in constant error carousel. In the recent years there has been a renewed interest in further improving on the basic architecture by modifying the functional form as seen with Gated Recurrent Units Cho et al. (2014), incorporating content-based soft atten- tion mechanisms Bahdanau et al. (2014); Weston et al. (2014), push-pop stacks Joulin & Mikolov (2015), or more generally external memory arrays with both content-based and relative addressing mechanisms Graves et al. (2014). In this work we focus the majority of our analysis on the LSTM due to its widespread popularity and a proven track record. Understanding Recurrent Networks . While there is an abundance of work that modiﬁes or extends the basic LSTM architecture, relatively little attention has been paid to understanding the properties of its representations and predictions. Greff et al. (2015) recently conducted a comprehensive study of LSTM components. Chung et al. evaluated GRU compared to LSTMs Chung et al. (2014). Joze- fowicz et al. (2015) conduct an automated architecture search of thousands of RNN architectures. Pascanu et al. (2013) examined the effects of depth . These approaches study recurrent network based only on the variations in the ﬁnal test set cross entropy, while we break down the performance into interpretable categories and study individual error types. Most related to our work is Hermans & Schrauwen (2013), who also study the long-term interactions learned by recurrent networks in the context of character-level language models, speciﬁcally in the context of parenthesis closing and time-scales analysis. Our work complements their results and provides additional types of analysis. Lastly, we are heavily inﬂuenced by work on in-depth analysis of errors in object detection Hoiem et al. (2012), where the ﬁnal mean average precision is similarly broken down and studied in detail. 3 E XPERIMENTAL SETUP We ﬁrst describe three commonly used recurrent network architectures (RNN, LSTM and the GRU), then describe their used in sequence learning and ﬁnally discuss the optimization. 3.1 R ECURRENT NEURAL NETWORK MODELS The simplest instantiation of a deep recurrent network arranges hidden state vectors hl tin a two- dimensional grid, where t= 1. . . T is thought of as time and l= 1. . . L is the depth. The bottom row of vectors h0 t=xtat depth zero holds the input vectors xtand each vector in the top row {hL t} is used to predict an output vector yt. All intermediate vectors hl tare computed with a recurrence formula based on hl t−1andhl−1 t. Through these hidden vectors, each output ytat time step t becomes a function of all input vectors up to t,{x1, . . . , x t}. The precise mathematical form of the recurrence (hl t−1,hl−1 t)→hl tvaries from model to model and we describe these details next. Vanilla Recurrent Neural Network (RNN) has a recurrence of the form hl t= tanh Wl( hl−1 t hl t−1) where we assume that all h∈Rn. The parameter matrix Wlon each layer has dimensions [ n×2n] andtanh is applied elementwise. Note that Wlvaries between layers but is shared through time. We omit the bias vectors for brevity. Interpreting the equation above, the inputs from the layer below in depth ( hl−1 t) and before in time ( hl t−1) are transformed and interact through additive interaction before being squashed by tanh . This is known to be a weak form of coupling Sutskever et al. (2011). Both the LSTM and the GRU (discussed next) include more powerful multiplicative interactions. Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997) was designed to address the difﬁculties of training RNNs Bengio et al. (1994). In particular, it was observed that the back- propagation dynamics caused the gradients in an RNN to either vanish or explode. It was later found that the exploding gradient concern can be alleviated with a heuristic of clipping the gradients at some maximum value Pascanu et al. (2012). On the other hand, LSTMs were designed to miti- gate the vanishing gradient problem. In addition to a hidden state vector hl t, LSTMs also maintain a 2\n",
      "\n",
      "DOCUMENT 3\n",
      "Under review as a conference paper at ICLR 2016 VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS Andrej Karpathy∗Justin Johnson∗Li Fei-Fei Department of Computer Science, Stanford University {karpathy,jcjohns,feifeili }@cs.stanford.edu ABSTRACT Recurrent Neural Networks (RNNs), and speciﬁcally a variant with Long Short- Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Us- ing character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with ﬁnite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study. 1 I NTRODUCTION Recurrent Neural Networks, and speciﬁcally a variant with Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997), have recently emerged as an effective model in a wide vari- ety of applications that involve sequential data. These include language modeling Mikolov et al. (2010), handwriting recognition and generation Graves (2013), machine translation Sutskever et al. (2014); Bahdanau et al. (2014), speech recognition Graves et al. (2013), video analysis Donahue et al. (2015) and image captioning Vinyals et al. (2015); Karpathy & Fei-Fei (2015). However, both the source of their impressive performance and their shortcomings remain poorly understood. This raises concerns of the lack of interpretability and limits our ability design better architectures. A few recent ablation studies analyzed the effects on performance as various gates and connections are removed Greff et al. (2015); Chung et al. (2014). However, while this analysis illuminates the performance-critical pieces of the architecture, it is still limited to examining the effects only on the global level of the ﬁnal test set perplexity alone. Similarly, an often cited ad- vantage of the LSTM architecture is that it can store and retrieve information over long time scales using its gating mechanisms, and this ability has been carefully studied in toy settings Hochreiter & Schmidhuber (1997). However, it is not immediately clear that similar mechanisms can be ef- fectively discovered and utilized by these networks in real-world data, and with the common use of simple stochastic gradient descent and truncated backpropagation through time. To our knowledge, our work provides the ﬁrst empirical exploration of the predictions of LSTMs and their learned representations on real-world data. Concretely, we use character-level language models as an interpretable testbed for illuminating the long-range dependencies learned by LSTMs. Our analysis reveals the existence of cells that robustly identify interpretable, high-level patterns such as line lengths, brackets and quotes. We further quantify the LSTM predictions with com- prehensive comparison to n-gram models, where we ﬁnd that LSTMs perform signiﬁcantly better on characters that require long-range reasoning. Finally, we conduct an error analysis in which we “peel the onion” of errors with a sequence of oracles. These results allow us to quantify the extent of remaining errors in several categories and to suggest speciﬁc areas for further study. ∗Both authors contributed equally to this work. 1arXiv:1506.02078v2 [cs.LG] 17 Nov 2015\n",
      "\n",
      "DOCUMENT 3\n",
      "Under review as a conference paper at ICLR 2016 VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS Andrej Karpathy∗Justin Johnson∗Li Fei-Fei Department of Computer Science, Stanford University {karpathy,jcjohns,feifeili }@cs.stanford.edu ABSTRACT Recurrent Neural Networks (RNNs), and speciﬁcally a variant with Long Short- Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Us- ing character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with ﬁnite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study. 1 I NTRODUCTION Recurrent Neural Networks, and speciﬁcally a variant with Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997), have recently emerged as an effective model in a wide vari- ety of applications that involve sequential data. These include language modeling Mikolov et al. (2010), handwriting recognition and generation Graves (2013), machine translation Sutskever et al. (2014); Bahdanau et al. (2014), speech recognition Graves et al. (2013), video analysis Donahue et al. (2015) and image captioning Vinyals et al. (2015); Karpathy & Fei-Fei (2015). However, both the source of their impressive performance and their shortcomings remain poorly understood. This raises concerns of the lack of interpretability and limits our ability design better architectures. A few recent ablation studies analyzed the effects on performance as various gates and connections are removed Greff et al. (2015); Chung et al. (2014). However, while this analysis illuminates the performance-critical pieces of the architecture, it is still limited to examining the effects only on the global level of the ﬁnal test set perplexity alone. Similarly, an often cited ad- vantage of the LSTM architecture is that it can store and retrieve information over long time scales using its gating mechanisms, and this ability has been carefully studied in toy settings Hochreiter & Schmidhuber (1997). However, it is not immediately clear that similar mechanisms can be ef- fectively discovered and utilized by these networks in real-world data, and with the common use of simple stochastic gradient descent and truncated backpropagation through time. To our knowledge, our work provides the ﬁrst empirical exploration of the predictions of LSTMs and their learned representations on real-world data. Concretely, we use character-level language models as an interpretable testbed for illuminating the long-range dependencies learned by LSTMs. Our analysis reveals the existence of cells that robustly identify interpretable, high-level patterns such as line lengths, brackets and quotes. We further quantify the LSTM predictions with com- prehensive comparison to n-gram models, where we ﬁnd that LSTMs perform signiﬁcantly better on characters that require long-range reasoning. Finally, we conduct an error analysis in which we “peel the onion” of errors with a sequence of oracles. These results allow us to quantify the extent of remaining errors in several categories and to suggest speciﬁc areas for further study. ∗Both authors contributed equally to this work. 1arXiv:1506.02078v2 [cs.LG] 17 Nov 2015\n",
      "\n",
      "DOCUMENT 5\n",
      "Under review as a conference paper at ICLR 2016 LSTM RNN GRU Layers 1 2 3 1 2 3 1 2 3 Size War and Peace Dataset 64 1.449 1.442 1.540 1.446 1.401 1.396 1.398 1.373 1.472 128 1.277 1.227 1.279 1.417 1.286 1.277 1.230 1.226 1.253 256 1.189 1.137 1.141 1.342 1.256 1.239 1.198 1.164 1.138 512 1.161 1.092 1.082 - - - 1.170 1.201 1.077 Linux Kernel Dataset 64 1.355 1.331 1.366 1.407 1.371 1.383 1.335 1.298 1.357 128 1.149 1.128 1.177 1.241 1.120 1.220 1.154 1.125 1.150 256 1.026 0.972 0.998 1.171 1.116 1.116 1.039 0.991 1.026 512 0.952 0.840 0.846 - - - 0.943 0.861 0.829 GRU­2 (512)GRU­3 (256)GRU­1 (128) LSTM­2 (512)LSTM­1 (64) GRU­1 (64) LSTM­2 (256)GRU­3 (128) RNN­2 (256)RNN­3 (128) GRU­1 (512)RNN­3 (64)RNN­1 (64) GRU­2 (128)LSTM­2 (64)RNN­1 (128) LSTM­3 (512)RNN­2 (64) GRU­2 (256)GRU­3 (64) LSTM­1 (256)LSTM­1 (128)LSTM­3 (128)GRU­2 (64) LSTM­3 (256) LSTM­1 (512)LSTM­3 (64) GRU­1 (256)RNN­2 (128) RNN­1 (256) RNN­3 (256)LSTM­2 (128) GRU­3 (512) Figure 1: Left: Thetest set cross-entropy loss for all models and datasets (low is good). Models in each row have nearly equal number of parameters. The test set has 300,000 characters. The standard deviation, estimated with 100 bootstrap samples, is less than 4×10−3in all cases. Right: A t-SNE embedding based on the probabilities assigned to test set characters by each model on War and Peace. The color, size, and marker correspond to model type, model size, and number of layers. 90/5/5 for LK. Therefore, there are approximately 300,000 characters in the validation/test splits in each case. The total number of characters in the vocabulary is 87 for WP and 101 for LK. 4.1 C OMPARING RECURRENT NETWORKS We ﬁrst train several recurrent network models to support further analysis and to compare their performance in a controlled setting. In particular, we train models in the cross product of type (LSTM/RNN/GRU), number of layers (1/2/3), number of parameters (4 settings), and both datasets (WP/KL). For a 1-layer LSTM we used hidden size vectors of 64,128,256, and 512 cells, which with our character vocabulary sizes translates to approximately 50K, 130K, 400K, and 1.3M parameters respectively. The sizes of hidden layers of the other models were carefully chosen so that the total number of parameters in each case is as close as possible to these 4 settings. The test set results are shown in Figure 1. Our consistent ﬁnding is that depth of at least two is beneﬁcial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both signiﬁcantly outperform the RNN. We also computed the fraction of times that each pair of models agree on the most likely character and use it to render a t-SNE Van der Maaten & Hinton (2008) embedding (we found this more stable and robust than the KL divergence). The plot (Figure 1, right) further supports the claim that the LSTM and the GRU make similar predictions while the RNNs form their own cluster. 4.2 I NTERNAL MECHANISMS OF AN LSTM Interpretable, long-range LSTM cells. An LSTMs can in principle use its memory cells to remem- ber long-range information and keep track of various attributes of text it is currently processing. For instance, it is a simple exercise to write down toy cell weights that would allow the cell to keep track of whether it is inside a quoted string. However, to our knowledge, the existence of such cells has never been experimentally demonstrated on real-world data. In particular, it could be argued that even if the LSTM is in principle capable of using these operations, practical optimization challenges (i.e. SGD dynamics, or approximate gradients due to truncated backpropagation through time) might prevent it from discovering these solutions. In this experiment we verify that multiple interpretable cells do in fact exist in these networks (see Figure 2). For instance, one cell is clearly acting as a line length counter, starting with a high value and then slowly decaying with each character until the next newline. Other cells turn on inside quotes, the parenthesis after if statements, inside strings or comments, or with increasing strength as the indentation of a block of code increases. In particular, note that truncated backpropagation with our hyperparameters prevents the gradient signal from di- rectly noticing dependencies longer than 100 characters, but we still observe cells that reliably keep track of quotes or comment blocks much longer than 100 characters (e.g. ∼230characters in the quote detection cell example in Figure 2). We hypothesize that these cells ﬁrst develop on patterns shorter than 100 characters but then also appropriately generalize to longer sequences. Gate activation statistics . We can gain some insight into the internal mechanisms of the LSTM by studying the gate activations in the networks as they process test set data. We were particularly interested in looking at the distributions of saturation regimes in the networks, where we deﬁne a 4\n"
     ]
    }
   ],
   "source": [
    "for document in answer1[\"source_documents\"]:\n",
    "    index_n = answer1[\"source_documents\"].index(document)\n",
    "    print(f\"\\nDOCUMENT {index_n + 1}\")\n",
    "    print(re.sub(r\"\\s+\", \" \", document.page_content.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b16f0387ce399",
   "metadata": {},
   "source": [
    "We can see that the first three chunks are the most relevant, while the last three don't really add that much to the answer.\n",
    "\n",
    "If we'd like, we can go a bit deeper with our answer. We can set up a memory for the last answer the LLM gave us so we can ask follow up questions. In this case, let's see if the LLM left out anything about PyCharm's debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233bb88309ba48d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:24.985020Z",
     "start_time": "2024-05-31T15:49:21.453330Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_history1 = [(answer1[\"query\"], answer1[\"result\"])]\n",
    "answer2 = pdf_qa_chain.invoke({\"query\": \"Have you left out any other types of debugging?\",\n",
    "                               \"chat_history\": chat_history1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5d632b16484d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:28.274380Z",
     "start_time": "2024-05-31T15:49:28.271332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there are other types of debugging mentioned in the context provided:\\n\\n1. Debugging JavaScript: This is mentioned as the next step intended for Professional edition users.\\n2. Debugging Django templates: It is mentioned that you have learned how to step through your template, evaluate expressions, and add watches in the context of a Django project.\\n3. Working in the Threads and Variables tab: It is mentioned that you can observe the variables used in the application by stepping through all the set breakpoints.\\n4. Working in the Console tab: It is mentioned that you can use the Console tab to see error messages or perform calculations not related to the current application.\\n\\nThese are the additional types of debugging mentioned in the context provided.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6922b4d1d9ba8fa",
   "metadata": {},
   "source": [
    "If our model is capable of it, we can even enter queries in a different language to the source documentation, and get relevant answers back in this language. Here we question our English-language documentation in German ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977361cabc240a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:38.271816Z",
     "start_time": "2024-05-31T15:49:35.254449Z"
    }
   },
   "outputs": [],
   "source": [
    "answer3 = pdf_qa_chain.invoke({\"query\": \"Wie kann man PyCharm installieren?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a8861805a7351",
   "metadata": {},
   "source": [
    "... and get a relevant answer in German!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc9df603729ace6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:49:56.958659Z",
     "start_time": "2024-05-31T15:49:56.955109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOCUMENT 1\n",
      "PyCharm 2024.1 Getting started/Installation guide Last modified: 06 May 2024 PyCharm is a cross-platform IDE that provides consistent experience on the Windows, macOS, and Linux operating systems. PyCharm is available in two editions: Professional, and Community. The Community edition is an open-source project, and it's free, but it has fewer features. The Professional edition is commercial, and provides an outstanding set of tools and features. For more information, refer to the editions comparison matrix ↗.Install PyCharm\n",
      "\n",
      "DOCUMENT 2\n",
      "You can install PyCharm using Toolbox or standalone installations. If you need assistance installing PyCharm, see the installation instructions: Install PyCharmRequirement Minimum Recommended Operating systemOfficially released versions of the following: Pre-release versions are not supported.The latest versions of the following: Microsoft Windows 10 1809 64-bit or later Windows Server 2019 64- bit or later• macOS 12.0 or later • Ubuntu Linux 20.04 LTS or a later LTS version that uses the following:• Gnome or KDE • X Window System (X11) Wayland support is in development. You can monitor the progress and leave your feedback in JBR-3206: Native Wayland support ↗.• GLIBC ↗ 2.29 or later •Windows 64-bit • macOS • Ubuntu Linux LTS •\n",
      "\n",
      "DOCUMENT 3\n",
      "To run PyCharm, find it in the Windows Start menu or use the desktop shortcut. You can also run the launcher batch script or executable in the installation directory under bin. When you run PyCharm for the first time, you can take several steps to complete the installation, customize your instance, and start working with the IDE. For more information, refer to Run PyCharm for the first time. For more information about the location of the default IDE directories with user- specific files, refer to Directories used by the IDE. Silent installation on Windows Silent installation is performed without any user interface. It can be used by network administrators to install PyCharm on a number of machines and avoid interrupting other users. To perform silent install, run the installer with the following switches:There is a separate installer for ARM64 processors. To verify the integrity of the installer, use the SHA checksum linked from the Download ↗ page. Run the installer and follow the wizard steps. Mind the following options in the installation wizard2. 64-bit launcher: Adds a launching icon to the Desktop. • Open Folder as Project: Adds an option to the folder context menu that will allow opening the selected directory as a PyCharm project.• .py: Establishes an association with Python files to open them in PyCharm. • Add launchers dir to the PATH: Allows running this PyCharm instance from the Console without specifying the path to it.•\n",
      "\n",
      "DOCUMENT 4\n",
      "PyCharm supports the following versions of Python: Install using the Toolbox App The JetBrains Toolbox App ↗ is the recommended tool to install JetBrains products. Use it to install and manage different products or several versions of the same product, including Early Access Program ↗ (EAP) and Nightly releases, update and roll back when necessary, and easily remove any tool. The Toolbox App maintains a list of all your projects to quickly open any project in the right IDE and version. Install the Toolbox AppPython 2: version 2.7 • Python 3: from the version 3.6 up to the version 3.12 • macOS Linux Download the installer .exe from the Toolbox App web page ↗. 1. Run the installer and follow the wizard steps. 2. After you run the Toolbox App, click its icon in the notification area and select which product you want to install. To install a specific version, click and select Available versions.3.Windows\n",
      "\n",
      "DOCUMENT 5\n",
      "PyCharm 2024.1 Getting started/Installation guide/Run PyCharm for the first time Last modified: 15 May 2024 You can use the Toolbox App to run any JetBrains product. In the case of a standalone installation, running PyCharm depends on the operating system: To run PyCharm, find it in the Windows Start menu or use the desktop shortcut. You can also run the launcher batch script or executable in the installation directory under bin. For more information about running PyCharm from the command line, refer to Command-line interface. You will see the Welcome screen, the starting point to your work with the IDE. This screen also appears when you close all opened projects. Use the tabs on the left side to switch to the specific welcome dialog.Run PyCharm for the first time macOS Linux Windows\n"
     ]
    }
   ],
   "source": [
    "for document in answer3[\"source_documents\"]:\n",
    "    index_n = answer3[\"source_documents\"].index(document)\n",
    "    print(f\"\\nDOCUMENT {index_n + 1}\")\n",
    "    print(re.sub(r\"\\s+\", \" \", document.page_content.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24653b6650739585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PyCharm kann über die Toolbox oder eigenständige Installationen installiert werden. Wenn du Hilfe bei der Installation benötigst, sieh dir die Installationsanweisungen an. Es gibt auch eine stille Installationsoption für Netzwerkadministratoren, um PyCharm auf mehreren Maschinen zu installieren, ohne andere Benutzer zu unterbrechen. Es gibt auch eine separate ARM64-Installationsdatei. Es wird empfohlen, die Integrität des Installationsprogramms mit dem SHA-Prüfsummenlink von der Downloadseite zu überprüfen.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer3[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
